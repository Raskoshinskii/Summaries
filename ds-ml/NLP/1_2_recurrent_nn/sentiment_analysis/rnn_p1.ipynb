{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0648ae",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using RNNs\n",
    "We have a dataset of combined reviews for films from IMDB and Rotten Tomatoes.\n",
    "\n",
    "- **Task**: build a model that can predict a sentiment (positive/negative) based on a user's review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "48999d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import (\n",
    "    Input, Lambda, Dropout,\n",
    "    Activation, Dense, Embedding,\n",
    "    SimpleRNN, Conv1D, GlobalMaxPooling1D,\n",
    "    MaxPooling1D, LSTM\n",
    ")\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e1707",
   "metadata": {},
   "source": [
    "### Data Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6673e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 23\n",
    "VALIDATION_SIZE = 0.1\n",
    "\n",
    "# parameters for vocab and embeddings\n",
    "VOCAB_SIZE = 10_000\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01d33bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Unique Comments (Train):  152610\n",
      "N Unique Comments (Test):  10660\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>To an entire generation of filmgoers, it just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Pixar classic is one of the best kids' movies ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Apesar de representar um imenso avanço tecnoló...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>When Woody perks up in the opening scene, it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Introduced not one but two indelible character...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  To an entire generation of filmgoers, it just ...\n",
       "1      1  Pixar classic is one of the best kids' movies ...\n",
       "2      1  Apesar de representar um imenso avanço tecnoló...\n",
       "3      1  When Woody perks up in the opening scene, it's...\n",
       "4      1  Introduced not one but two indelible character..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/jovyan/work/data_sets/uds_sentiment_analysis/movie_reviews_train.csv', header=[0])\n",
    "test_df = pd.read_csv('/home/jovyan/work/data_sets/uds_sentiment_analysis/movie_reviews_test.csv', header=[0])\n",
    "\n",
    "print('N Unique Comments (Train): ', train_df.shape[0])\n",
    "print('N Unique Comments (Test): ', test_df.shape[0])\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d886b",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- Ordinary preprocessing (removing stopwords, symbols, text normalization)\n",
    "- Feature Generation (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f677f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# class distribution\n",
    "train_df['label'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e69cdb",
   "metadata": {},
   "source": [
    "- **Not heavily imbalanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a91db036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1    0.587496\n",
       " 0    0.412504\n",
       " Name: label, dtype: float64,\n",
       " 1    0.587511\n",
       " 0    0.412489\n",
       " Name: label, dtype: float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df['text'], train_df['label'],\n",
    "    test_size=VALIDATION_SIZE, random_state=RANDOM_SEED,\n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "# look at obtained labels distribution\n",
    "(\n",
    "    y_train.value_counts(normalize=True),\n",
    "    y_val.value_counts(normalize=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbaaae3",
   "metadata": {},
   "source": [
    "- We have **152k unique comments** in training data\n",
    "- We can either train own `word2vec` model or download `pretrained embeddings`\n",
    "\n",
    "In this example we will [download pretrained embeddings](http://nlp.stanford.edu/data/glove.6B.zip) (`glove 50, 6B`)\n",
    "\n",
    "**Transform Words into Vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7700796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw review:  If the picture overcrowds its screen, at least we must admit it is an impressive kaleidoscope; and probably nothing short of that could reflect the gaudy career of America's foremost showman.\n"
     ]
    }
   ],
   "source": [
    "# initialize vocab (fit on train)\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='#$%&()*+-/:;<=>@[\\\\]^{|}~\\t\\n,.!\"')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "print('Raw review: ', X_train.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36999194",
   "metadata": {},
   "source": [
    "`Tokenizer` from `keras.preprocessing.text` has the following logic:\n",
    "- First it's fit on all text data, then it computes token frequencies and ranks them \n",
    "- Token that has index 1 has the most word frequency, then second, ...\n",
    "- Parameter `num_words` defines number of words to keep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b69d67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \"super\" appears 946 times\n",
      "Word \"super\" has index: 1269 \n"
     ]
    }
   ],
   "source": [
    "# tokenizer has info about our vocab\n",
    "word = 'super'\n",
    "\n",
    "word_indx = tokenizer.word_index[word]\n",
    "word_count = tokenizer.word_counts[word]\n",
    "\n",
    "print(f'Word \"{word}\" appears {word_count} times')\n",
    "print(f'Word \"{word}\" has index: {word_indx} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bf2f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 686911\n",
      "a: 352446\n",
      "and: 345069\n",
      "of: 314398\n",
      "to: 274789\n"
     ]
    }
   ],
   "source": [
    "# look at the most popular words\n",
    "n = 5\n",
    "top_n_word_counts = list(tokenizer.word_index.keys())[:n]\n",
    "for word in top_n_word_counts:\n",
    "    print(f'{word}: {tokenizer.word_counts[word]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03960cc7",
   "metadata": {},
   "source": [
    "- Most word counts have words that should be in stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ac62be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed review: \n",
      " [42, 1, 344, 57, 240, 29, 219, 77, 220, 1069, 8, 6, 28, 1007, 3, 243, 161, 338, 4, 11, 97, 4751, 1, 557, 4, 3797, 7133]\n"
     ]
    }
   ],
   "source": [
    "# transform text into sequence of vocab indexes (index -> token)\n",
    "X_train_enc = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_enc = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "print('Transformed review: \\n', X_train_enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655b954f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81067     2109\n",
       "20034     2084\n",
       "69615     1922\n",
       "52714     1876\n",
       "134956    1732\n",
       "          ... \n",
       "100477       0\n",
       "53995        0\n",
       "96621        0\n",
       "45131        0\n",
       "17490        0\n",
       "Length: 137349, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at sequence length distribution\n",
    "seq_len_dist = pd.Series(X_train_enc).apply(len).sort_values(ascending=False)\n",
    "seq_len_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46de19a",
   "metadata": {},
   "source": [
    "- Some reviews have zero length -> probably words don't appear in vocab\n",
    "- Probably it's better to exclude such observations -> they don't provide any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2b1bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally: exclude zero seq len texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d6a4a",
   "metadata": {},
   "source": [
    "- Each text has different length. Howeve, we must align it and make sure that **all texts have the same length**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f08639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed review (Length Restricted): \n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0   42\n",
      "    1  344   57  240   29  219   77  220 1069    8    6   28 1007    3\n",
      "  243  161  338    4   11   97 4751    1  557    4 3797 7133]\n"
     ]
    }
   ],
   "source": [
    "# restrict each sequence by requred length\n",
    "X_train_enc = pad_sequences(X_train_enc, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_val_enc = pad_sequences(X_val_enc, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Transformed review (Length Restricted): \\n', X_train_enc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcbb382",
   "metadata": {},
   "source": [
    "### Pretrained Embeddings Retrival\n",
    "Let's download pretrained Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ce93f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_glove_embeddings(f_path):\n",
    "    \"\"\"Downloads pretrained embeddings.\"\"\"\n",
    "    embeddings = {}\n",
    "    with open(f_path) as f:\n",
    "        # get the total number of lines in the file for tqdm\n",
    "        total_lines = sum(1 for line in f)\n",
    "        f.seek(0)  # reset file pointer to the beginning\n",
    "        \n",
    "        for line in tqdm(f, total=total_lines, desc=\"Downloading Glove Embeddings\"):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = coefs\n",
    "        del values, word, coefs, line\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1545fcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Glove Embeddings: 100%|██████████████████████████████████████████| 400000/400000 [00:12<00:00, 32722.05it/s]\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings = download_glove_embeddings(\n",
    "    f_path='/home/jovyan/work/data_sets/uds_sentiment_analysis/glove.6B/glove.6B.50d.txt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f8ff0c",
   "metadata": {},
   "source": [
    "- Unfortunately, not all words from downloaded pretrained embeddings are available for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41f5d6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Missing Embeddings 251\n",
      "N Missing Embeddings  2.51 %\n"
     ]
    }
   ],
   "source": [
    "# look at vocab overlap\n",
    "first_10k = {k: v for k, v in tokenizer.word_index.items() if v < 10000}\n",
    "\n",
    "first_10k_words_set = set(first_10k.keys())\n",
    "glove_embeddings_words_set = set(glove_embeddings.keys())\n",
    "\n",
    "missing_embeddings = first_10k_words_set.difference(glove_embeddings_words_set)\n",
    "print(f'N Missing Embeddings {len(missing_embeddings)}')\n",
    "print(f'N Missing Embeddings ', round(len(missing_embeddings)/10_000*100,2), '%')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbe0c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained Embedding Matrix Shape:  (10000, 50)\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix from Tokenizer: rows -> word indexes\n",
    "embedding_matrix = np.zeros((tokenizer.num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in first_10k.items():\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Obtained Embedding Matrix Shape: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6decdc4c",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "RNNs help to catch/understand a pattern that **depends on time or order.** For example, when we are trying to classify an episode from a film, it is important for us to know that there were a couple of episodes earlier, or in order to understand the meaning of a certain word, we need to know the context that came before it.\n",
    "\n",
    "**Simple RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcbe991f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:21:41.892821: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-04-16 18:21:41.892903: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-04-16 18:21:41.892923: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c05a32572f15): /proc/driver/nvidia/version does not exist\n",
      "2024-04-16 18:21:41.894162: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 515,201\n",
      "Trainable params: 15,201\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:21:42.105668: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2024-04-16 18:21:42.105729: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2024-04-16 18:21:42.106985: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "NAME = \"simple_rnn\"\n",
    "MONITOR_METRIC = 'val_acc'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# initialize embedding layer \n",
    "embedding_layer = Embedding(\n",
    "    input_dim=tokenizer.num_words,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False # we don't want to train new embeddings because we already downloaded pretrained!\n",
    ")\n",
    "                            \n",
    "simple_rnn = Sequential()\n",
    "simple_rnn.add(embedding_layer)\n",
    "simple_rnn.add(SimpleRNN(100))\n",
    "simple_rnn.add(Dense(1))\n",
    "simple_rnn.add(Activation('sigmoid'))\n",
    "\n",
    "# initialize callbacks\n",
    "# automatic learning curves building\n",
    "callback_1 = TensorBoard(\n",
    "    log_dir='./logs/logs_{}'.format(NAME),\n",
    "    histogram_freq=0,\n",
    "    write_graph=False,\n",
    "    write_images=False\n",
    ")\n",
    "\n",
    "# EarlyStopping callback\n",
    "callback_2 = EarlyStopping(\n",
    "    monitor=MONITOR_METRIC, min_delta=0, patience=5, verbose=0, mode='auto'\n",
    ")\n",
    "\n",
    "\n",
    "# best model callback\n",
    "callback_3 = ModelCheckpoint(\n",
    "    filepath=\"../models/model_{}.hdf5\".format(NAME),\n",
    "    monitor=MONITOR_METRIC,\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "simple_rnn.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "simple_rnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d2ecb9",
   "metadata": {},
   "source": [
    "- Since we don't train first embedding layer we have 500k non trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c7d8cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:21:44.831920: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/135 [..............................] - ETA: 11s - loss: 0.7103 - accuracy: 0.5190 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:21:45.809453: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2024-04-16 18:21:45.809495: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2024-04-16 18:21:45.895189: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2024-04-16 18:21:45.903008: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2024-04-16 18:21:45.921005: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45\n",
      "\n",
      "2024-04-16 18:21:45.935596: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45/c05a32572f15.trace.json.gz\n",
      "2024-04-16 18:21:45.945871: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45\n",
      "\n",
      "2024-04-16 18:21:45.949017: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45/c05a32572f15.memory_profile.json.gz\n",
      "2024-04-16 18:21:45.966437: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45\n",
      "Dumped tool data for xplane.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45/c05a32572f15.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45/c05a32572f15.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45/c05a32572f15.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45/c05a32572f15.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_21_45/c05a32572f15.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 [==============================] - 11s 74ms/step - loss: 0.6167 - accuracy: 0.6504 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 12s 91ms/step - loss: 0.5823 - accuracy: 0.6888 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 12s 90ms/step - loss: 0.5633 - accuracy: 0.7050 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 10s 71ms/step - loss: 0.5562 - accuracy: 0.7097 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 10s 76ms/step - loss: 0.5458 - accuracy: 0.7183 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 10s 75ms/step - loss: 0.5333 - accuracy: 0.7260 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 11s 80ms/step - loss: 0.5231 - accuracy: 0.7352 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 10s 75ms/step - loss: 0.5255 - accuracy: 0.7322 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 10s 72ms/step - loss: 0.5151 - accuracy: 0.7408 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 10s 73ms/step - loss: 0.5136 - accuracy: 0.7421 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 10s 71ms/step - loss: 0.5062 - accuracy: 0.7458 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 10s 72ms/step - loss: 0.5015 - accuracy: 0.7489 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 10s 71ms/step - loss: 0.5002 - accuracy: 0.7497 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 10s 71ms/step - loss: 0.5071 - accuracy: 0.7443 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 10s 74ms/step - loss: 0.5007 - accuracy: 0.7495 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 10s 75ms/step - loss: 0.4916 - accuracy: 0.7550 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 10s 71ms/step - loss: 0.4912 - accuracy: 0.7562 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 10s 72ms/step - loss: 0.4861 - accuracy: 0.7601 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 10s 72ms/step - loss: 0.4855 - accuracy: 0.7617 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 12s 87ms/step - loss: 0.4837 - accuracy: 0.7610 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2c91c0670>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_rnn.fit(\n",
    "    X_train_enc, y_train,\n",
    "    validation_data=[X_val_enc, y_val], \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback_1, callback_2, callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9456749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 s, sys: 12 s, total: 43.9 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# rnn inference\n",
    "pred_proba_train_rnn = simple_rnn.predict(X_train_enc)\n",
    "pred_proba_val_rnn = simple_rnn.predict(X_val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef8ccbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (Train):  0.8455963238135913\n",
      "ROC-AUC (Validation):  0.8312706177799565\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.57      0.65      6295\n",
      "           1       0.74      0.87      0.80      8966\n",
      "\n",
      "    accuracy                           0.75     15261\n",
      "   macro avg       0.75      0.72      0.73     15261\n",
      "weighted avg       0.75      0.75      0.74     15261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "pre_labels_val = np.where(pred_proba_val_rnn > threshold, 1, 0)\n",
    "\n",
    "roc_auc_train_rnn = roc_auc_score(y_train, pred_proba_train_rnn)\n",
    "roc_auc_val_rnn = roc_auc_score(y_val, pred_proba_val_rnn)\n",
    "class_report_rnn = classification_report(y_val, pre_labels_val)\n",
    "\n",
    "print('ROC-AUC (Train): ', roc_auc_train_rnn)\n",
    "print('ROC-AUC (Validation): ', roc_auc_val_rnn)\n",
    "\n",
    "print('\\n', class_report_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5972439d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[94.747   ],\n",
       "       [ 3.217858],\n",
       "       [63.054295],\n",
       "       [14.95009 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom testing\n",
    "reviews_to_test = pd.Series([\n",
    "    'The film was awesome. I liked the plot and actors!',\n",
    "    'The film was bad. Such a boring and awful! Not Recommend!',\n",
    "    'I love it!',\n",
    "    'Not Recommend and do not waste your time',\n",
    "])\n",
    "\n",
    "reviews_vec = tokenizer.texts_to_sequences(reviews_to_test)\n",
    "reviews_vec = pad_sequences(reviews_vec, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "simple_rnn.predict(reviews_vec)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546c38b",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory (LSTM)\n",
    "LSTM has a number of advantages over a simple RNN. LSTM is **able to store the necessary information** about a certain object and not pay attention to irrelevant information. For example, a scene without mentioning the main character will not change information about him and, on the contrary, when mentioned, it will be focused.\n",
    "\n",
    "**Forgetting Mechanism**\n",
    "\n",
    "If the episode ends, for example, the model should forget the current location, time of day, and reset any information about the specific scene. However, if a character dies in an episode, the network must continue to remember that he is no longer alive.\n",
    "In the end the model must know what is important and what is not when a new input appears.\n",
    "\n",
    "**Remembering Mechanism**\n",
    "\n",
    "When the model sees a new episode, it needs to decide whether to use and save any information about it. For example, we have seen a new meme, should we remeber it? Thus, when a new input comes, model first forgets long-term information. Then during learning it finds out what part of new data it must put in long-term memory. Later during training it decides what part of long-term memory important and must be used and what info is not important and might be forgotten.\n",
    "\n",
    "\n",
    "### Advantage over RNN\n",
    "RNN always rewrite memory whereas LSTM is more flexible and is able to store some info in long-term memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cc56fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 560,501\n",
      "Trainable params: 60,501\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NAME = \"simple_lstm\"\n",
    "MONITOR_METRIC = 'val_acc'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    tokenizer.num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False\n",
    ")\n",
    "                            \n",
    "simple_lstm = Sequential()\n",
    "simple_lstm.add(embedding_layer)\n",
    "simple_lstm.add(LSTM(100))\n",
    "simple_lstm.add(Dense(1))\n",
    "simple_lstm.add(Activation('sigmoid'))\n",
    "\n",
    "simple_lstm.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "simple_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e13015c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  1/135 [..............................] - ETA: 3:12 - loss: 0.8293 - accuracy: 0.4141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:25:30.087811: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2024-04-16 18:25:30.087868: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  2/135 [..............................] - ETA: 35s - loss: 0.7904 - accuracy: 0.4175 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:25:30.386412: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2024-04-16 18:25:30.400826: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2024-04-16 18:25:30.426955: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30\n",
      "\n",
      "2024-04-16 18:25:30.446879: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30/c05a32572f15.trace.json.gz\n",
      "2024-04-16 18:25:30.456726: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30\n",
      "\n",
      "2024-04-16 18:25:30.459396: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30/c05a32572f15.memory_profile.json.gz\n",
      "2024-04-16 18:25:30.478317: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30\n",
      "Dumped tool data for xplane.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30/c05a32572f15.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30/c05a32572f15.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30/c05a32572f15.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30/c05a32572f15.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_25_30/c05a32572f15.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 [==============================] - 34s 240ms/step - loss: 0.6040 - accuracy: 0.6636 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 32s 237ms/step - loss: 0.5470 - accuracy: 0.7158 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 32s 237ms/step - loss: 0.5235 - accuracy: 0.7330 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 32s 235ms/step - loss: 0.5103 - accuracy: 0.7424 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 32s 234ms/step - loss: 0.4979 - accuracy: 0.7520 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 32s 235ms/step - loss: 0.4886 - accuracy: 0.7580 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 33s 241ms/step - loss: 0.4780 - accuracy: 0.7643 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 31s 232ms/step - loss: 0.4717 - accuracy: 0.7681 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 33s 246ms/step - loss: 0.4651 - accuracy: 0.7726 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 34s 251ms/step - loss: 0.4588 - accuracy: 0.7763 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 32s 235ms/step - loss: 0.4554 - accuracy: 0.7780 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 32s 236ms/step - loss: 0.4481 - accuracy: 0.7825 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 33s 241ms/step - loss: 0.4414 - accuracy: 0.7861 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 33s 244ms/step - loss: 0.4384 - accuracy: 0.7881 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 32s 235ms/step - loss: 0.4328 - accuracy: 0.7918 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 32s 239ms/step - loss: 0.4271 - accuracy: 0.7951 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 32s 236ms/step - loss: 0.4217 - accuracy: 0.7988 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 32s 235ms/step - loss: 0.4147 - accuracy: 0.8029 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 34s 249ms/step - loss: 0.4107 - accuracy: 0.8046 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 33s 241ms/step - loss: 0.4061 - accuracy: 0.8070 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2a8256d90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_lstm.fit(\n",
    "    X_train_enc, y_train,\n",
    "    validation_data=[X_val_enc, y_val], \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback_1, callback_2, callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10b0c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 30s, sys: 46 s, total: 2min 16s\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# lstm inference\n",
    "pred_proba_train_lstm = simple_lstm.predict(X_train_enc)\n",
    "pred_proba_val_lstm = simple_lstm.predict(X_val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5463a28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (Train):  0.8976642858370705\n",
      "ROC-AUC (Validation):  0.8610103529404262\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.69      0.72      6295\n",
      "           1       0.80      0.84      0.82      8966\n",
      "\n",
      "    accuracy                           0.78     15261\n",
      "   macro avg       0.78      0.77      0.77     15261\n",
      "weighted avg       0.78      0.78      0.78     15261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "pre_labels_val = np.where(pred_proba_val_lstm > threshold, 1, 0)\n",
    "\n",
    "roc_auc_train_lstm = roc_auc_score(y_train, pred_proba_train_lstm)\n",
    "roc_auc_val_lstm = roc_auc_score(y_val, pred_proba_val_lstm)\n",
    "class_report_lstm = classification_report(y_val, pre_labels_val)\n",
    "\n",
    "print('ROC-AUC (Train): ', roc_auc_train_lstm)\n",
    "print('ROC-AUC (Validation): ', roc_auc_val_lstm)\n",
    "\n",
    "print('\\n', class_report_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb9d082a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67.68521  ],\n",
       "       [ 4.2813063],\n",
       "       [73.819824 ],\n",
       "       [33.777557 ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom testing (test the same observations to compare predicted probability)\n",
    "reviews_to_test = pd.Series([\n",
    "    'The film was awesome. I liked the plot and actors!',\n",
    "    'The film was bad. Such a boring and awful! Not Recommend!',\n",
    "    'I love it!',\n",
    "    'Not Recommend and do not waste your time',\n",
    "])\n",
    "\n",
    "reviews_vec = tokenizer.texts_to_sequences(reviews_to_test)\n",
    "reviews_vec = pad_sequences(reviews_vec, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "simple_lstm.predict(reviews_vec)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81ce433",
   "metadata": {},
   "source": [
    "### Modified LSTM\n",
    "Let's try to improve the performance of baseline LSTM model. In practice, the following parameters work in the best way:\n",
    "- Dropout (less overfitting on certain words)\n",
    "- Masking in Embeddings (in this case the loss doesn't take into account 0 when review is padded)\n",
    "- L1/L2 regularization (works not so well in practice, LSTM alredy prevents gradient exploding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab98c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 560,501\n",
      "Trainable params: 60,501\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NAME = \"modified_lstm\"\n",
    "MONITOR_METRIC = 'val_acc'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    tokenizer.num_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LENGTH,\n",
    "    trainable=False,\n",
    "    mask_zero=True\n",
    ")\n",
    "                            \n",
    "modified_lstm = Sequential()\n",
    "modified_lstm.add(embedding_layer)\n",
    "modified_lstm.add(Dropout(0.2))\n",
    "modified_lstm.add(LSTM(100, dropout=0.1, recurrent_dropout=0.1))\n",
    "modified_lstm.add(Dropout(0.2))\n",
    "modified_lstm.add(Dense(1))\n",
    "modified_lstm.add(Activation('sigmoid'))\n",
    "\n",
    "modified_lstm.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "modified_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88be3b1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  1/135 [..............................] - ETA: 5:38 - loss: 0.6911 - accuracy: 0.5293"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:37:54.477082: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2024-04-16 18:37:54.477126: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  2/135 [..............................] - ETA: 50s - loss: 0.6919 - accuracy: 0.5361 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:37:54.922518: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2024-04-16 18:37:54.955394: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2024-04-16 18:37:55.021079: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54\n",
      "\n",
      "2024-04-16 18:37:55.085711: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54/c05a32572f15.trace.json.gz\n",
      "2024-04-16 18:37:55.112293: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54\n",
      "\n",
      "2024-04-16 18:37:55.114551: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54/c05a32572f15.memory_profile.json.gz\n",
      "2024-04-16 18:37:55.144713: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54\n",
      "Dumped tool data for xplane.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54/c05a32572f15.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54/c05a32572f15.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54/c05a32572f15.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54/c05a32572f15.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_37_54/c05a32572f15.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 [==============================] - 56s 401ms/step - loss: 0.6047 - accuracy: 0.6644 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 51s 378ms/step - loss: 0.5711 - accuracy: 0.6963 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 49s 361ms/step - loss: 0.5529 - accuracy: 0.7107 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 49s 366ms/step - loss: 0.5420 - accuracy: 0.7186 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 48s 354ms/step - loss: 0.5319 - accuracy: 0.7246 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 48s 356ms/step - loss: 0.5272 - accuracy: 0.7302 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 49s 365ms/step - loss: 0.5205 - accuracy: 0.7338 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 59s 435ms/step - loss: 0.5163 - accuracy: 0.7382 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 51s 378ms/step - loss: 0.5124 - accuracy: 0.7387 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 48s 357ms/step - loss: 0.5082 - accuracy: 0.7415 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 50s 371ms/step - loss: 0.5050 - accuracy: 0.7443 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 48s 356ms/step - loss: 0.5027 - accuracy: 0.7469 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 49s 362ms/step - loss: 0.4971 - accuracy: 0.7485 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 50s 371ms/step - loss: 0.4950 - accuracy: 0.7501 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 50s 373ms/step - loss: 0.4923 - accuracy: 0.7524 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 53s 390ms/step - loss: 0.4905 - accuracy: 0.7545 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 49s 366ms/step - loss: 0.4875 - accuracy: 0.7557 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 47s 347ms/step - loss: 0.4860 - accuracy: 0.7573 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 49s 362ms/step - loss: 0.4819 - accuracy: 0.7584 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 54s 401ms/step - loss: 0.4811 - accuracy: 0.7598 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2a82d39d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_lstm.fit(\n",
    "    X_train_enc, y_train,\n",
    "    validation_data=[X_val_enc, y_val], \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback_1, callback_2, callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e816e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 42.7 s, total: 2min 55s\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# lstm inference\n",
    "pred_proba_train_lstm = modified_lstm.predict(X_train_enc)\n",
    "pred_proba_val_lstm = modified_lstm.predict(X_val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11230e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (Train):  0.8694888480839094\n",
      "ROC-AUC (Validation):  0.8547924672449817\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.64      0.70      6295\n",
      "           1       0.77      0.86      0.81      8966\n",
      "\n",
      "    accuracy                           0.77     15261\n",
      "   macro avg       0.77      0.75      0.76     15261\n",
      "weighted avg       0.77      0.77      0.77     15261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "pre_labels_val = np.where(pred_proba_val_lstm > threshold, 1, 0)\n",
    "\n",
    "roc_auc_train_lstm = roc_auc_score(y_train, pred_proba_train_lstm)\n",
    "roc_auc_val_lstm = roc_auc_score(y_val, pred_proba_val_lstm)\n",
    "class_report_lstm = classification_report(y_val, pre_labels_val)\n",
    "\n",
    "print('ROC-AUC (Train): ', roc_auc_train_lstm)\n",
    "print('ROC-AUC (Validation): ', roc_auc_val_lstm)\n",
    "\n",
    "print('\\n', class_report_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6b058b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67.68521  ],\n",
       "       [ 4.2813063],\n",
       "       [73.819824 ],\n",
       "       [33.777557 ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom testing (test the same observations to compare predicted probability)\n",
    "reviews_to_test = pd.Series([\n",
    "    'The film was awesome. I liked the plot and actors!',\n",
    "    'The film was bad. Such a boring and awful! Not Recommend!',\n",
    "    'I love it!',\n",
    "    'Not Recommend and do not waste your time',\n",
    "])\n",
    "\n",
    "reviews_vec = tokenizer.texts_to_sequences(reviews_to_test)\n",
    "reviews_vec = pad_sequences(reviews_vec, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "simple_lstm.predict(reviews_vec)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081139c0",
   "metadata": {},
   "source": [
    "### Convolutional Neural Net\n",
    "We can add convolutional layers into RNNs. The main issue: overfitting on little data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18c05904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 40, 50)            500000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 37, 100)           20100     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34, 100)           40100     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 31, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 620,601\n",
      "Trainable params: 620,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NAME = \"lstm_cnn\"\n",
    "MONITOR_METRIC = 'val_acc'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# initialize model\n",
    "lstm_cnn = Sequential()\n",
    "lstm_cnn.add(Embedding(tokenizer.num_words, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=True))\n",
    "lstm_cnn.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "lstm_cnn.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "lstm_cnn.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "lstm_cnn.add(GlobalMaxPooling1D())\n",
    "lstm_cnn.add(Dense(100, activation='relu'))\n",
    "lstm_cnn.add(Dropout(0.5))\n",
    "lstm_cnn.add(Dense(100, activation='relu'))\n",
    "lstm_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_cnn.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "lstm_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9df93854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  2/135 [..............................] - ETA: 18s - loss: 0.6901 - accuracy: 0.5737 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:58:35.198387: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2024-04-16 18:58:35.198430: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2024-04-16 18:58:35.369114: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2024-04-16 18:58:35.370408: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2024-04-16 18:58:35.377714: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35\n",
      "\n",
      "2024-04-16 18:58:35.381176: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35/c05a32572f15.trace.json.gz\n",
      "2024-04-16 18:58:35.385394: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35\n",
      "\n",
      "2024-04-16 18:58:35.388037: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35/c05a32572f15.memory_profile.json.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  3/135 [..............................] - ETA: 23s - loss: 0.6877 - accuracy: 0.5843"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 18:58:35.406222: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35\n",
      "Dumped tool data for xplane.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35/c05a32572f15.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35/c05a32572f15.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35/c05a32572f15.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35/c05a32572f15.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_18_58_35/c05a32572f15.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 [==============================] - 20s 141ms/step - loss: 0.5766 - accuracy: 0.6815 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/5\n",
      "135/135 [==============================] - 19s 139ms/step - loss: 0.4157 - accuracy: 0.8087 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/5\n",
      "135/135 [==============================] - 21s 158ms/step - loss: 0.3663 - accuracy: 0.8373 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/5\n",
      "135/135 [==============================] - 21s 156ms/step - loss: 0.3222 - accuracy: 0.8603 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/5\n",
      "135/135 [==============================] - 23s 167ms/step - loss: 0.2700 - accuracy: 0.8893 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff268ec5220>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_cnn.fit(\n",
    "    X_train_enc, y_train,\n",
    "    validation_data=[X_val_enc, y_val], \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=5, # set 5 epochs because prone to overfitting\n",
    "    callbacks=[callback_1, callback_2, callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1801141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.3 s, sys: 8.75 s, total: 51 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# lstm inference\n",
    "pred_proba_train_lstm = lstm_cnn.predict(X_train_enc)\n",
    "pred_proba_val_lstm = lstm_cnn.predict(X_val_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37535942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (Train):  0.9792408073529836\n",
      "ROC-AUC (Validation):  0.8746965280717182\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75      6295\n",
      "           1       0.82      0.84      0.83      8966\n",
      "\n",
      "    accuracy                           0.79     15261\n",
      "   macro avg       0.79      0.79      0.79     15261\n",
      "weighted avg       0.79      0.79      0.79     15261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "pre_labels_val = np.where(pred_proba_val_lstm > threshold, 1, 0)\n",
    "\n",
    "roc_auc_train_lstm = roc_auc_score(y_train, pred_proba_train_lstm)\n",
    "roc_auc_val_lstm = roc_auc_score(y_val, pred_proba_val_lstm)\n",
    "class_report_lstm = classification_report(y_val, pre_labels_val)\n",
    "\n",
    "print('ROC-AUC (Train): ', roc_auc_train_lstm)\n",
    "print('ROC-AUC (Validation): ', roc_auc_val_lstm)\n",
    "\n",
    "print('\\n', class_report_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ae7e2",
   "metadata": {},
   "source": [
    "### RNN on Symbols \n",
    "\n",
    "**What Preprocessing is Required for Symbols?**\n",
    "- similar to words, we have to train embeddings for symbols\n",
    "- represent symbols as OHE embeddings\n",
    "\n",
    "**What Symbols to Include?**\n",
    "- common practice (english) -> 70 symbols: lower case letters, numbers(1, 2, ..., 9) and punctuation\n",
    "- depending on a task, symbols length might be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "809853a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercase letters:  abcdefghijklmnopqrstuvwxyz\n",
      "digits:  0123456789\n",
      "punctuation:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "ascii_lowercase = string.ascii_lowercase\n",
    "digits = string.digits\n",
    "punctuation = string.punctuation\n",
    "\n",
    "print('lowercase letters: ', ascii_lowercase)\n",
    "print('digits: ', digits)\n",
    "print('punctuation: ', punctuation)\n",
    "\n",
    "# create char vocab\n",
    "def get_char_vocab():\n",
    "    alphabet = (\n",
    "        list(string.ascii_lowercase) + \n",
    "        list(string.digits) + \n",
    "        list(string.punctuation) +\n",
    "        [' ', '\\n']\n",
    "    )\n",
    "    char_vocab = {\n",
    "        char: i + 1 for i, char  in enumerate(alphabet)\n",
    "    }\n",
    "    vocab_size = len(char_vocab.keys())\n",
    "    return char_vocab, vocab_size\n",
    "\n",
    "def text2char_seq(text: list, char_vocab: dict):\n",
    "    seq = []\n",
    "    for char in text:\n",
    "        char = char_vocab.get(char, None)\n",
    "        if char:\n",
    "            seq.append(char)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "557473d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  to an entire generation of filmgoers, it just might represent the most significant leap in storytelling that they will ever see...\n",
      "\n",
      "Character Sequence Encoded: \n",
      " [20, 15, 69, 1, 14, 69, 5, 14, 20, 9, 18, 5, 69, 7, 5, 14, 5, 18, 1, 20, 9, 15, 14, 69, 15, 6, 69, 6, 9, 12, 13, 7, 15, 5, 18, 19, 48, 69, 9, 20, 69, 10, 21, 19, 20, 69, 13, 9, 7, 8, 20, 69, 18, 5, 16, 18, 5, 19, 5, 14, 20, 69, 20, 8, 5, 69, 13, 15, 19, 20, 69, 19, 9, 7, 14, 9, 6, 9, 3, 1, 14, 20, 69, 12, 5, 1, 16, 69, 9, 14, 69, 19, 20, 15, 18, 25, 20, 5, 12, 12, 9, 14, 7, 69, 20, 8, 1, 20, 69, 20, 8, 5, 25, 69, 23, 9, 12, 12, 69, 5, 22, 5, 18, 69, 19, 5, 5, 50, 50, 50]\n"
     ]
    }
   ],
   "source": [
    "# test transformation\n",
    "vocab, vocab_size  = get_char_vocab()\n",
    "sentence_char_seq_enc = text2char_seq(X_train[0].lower(), char_vocab=vocab)\n",
    "\n",
    "print('Original Text: ', X_train[0].lower())\n",
    "print('\\nCharacter Sequence Encoded: \\n', sentence_char_seq_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5b28dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.3 s, sys: 316 ms, total: 20.6 s\n",
      "Wall time: 20.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# apply character transformation\n",
    "X_train_char_enc = X_train.str.lower().apply(lambda x: text2char_seq(x, vocab))\n",
    "X_val_char_enc = X_val.str.lower().apply(lambda x: text2char_seq(x, vocab))\n",
    "\n",
    "# align vectors using padding\n",
    "X_train_char_enc = pad_sequences(X_train_char_enc, maxlen=MAX_SEQUENCE_LENGTH, value=0)\n",
    "X_val_char_enc = pad_sequences(X_val_char_enc, maxlen=MAX_SEQUENCE_LENGTH, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ca5a9",
   "metadata": {},
   "source": [
    "**Char Embedding CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cb7ef696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 50)            3550      \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 37, 100)           20100     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 34, 100)           40100     \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 31, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 124,151\n",
      "Trainable params: 124,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NAME = \"char_cnn\"\n",
    "MONITOR_METRIC = 'val_acc'\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "char_cnn = Sequential()\n",
    "char_cnn.add(Embedding(vocab_size+1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=True))\n",
    "char_cnn.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "char_cnn.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "char_cnn.add(Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\"))\n",
    "char_cnn.add(GlobalMaxPooling1D())\n",
    "char_cnn.add(Dense(100, activation='relu'))\n",
    "char_cnn.add(Dropout(0.5))\n",
    "char_cnn.add(Dense(100, activation='relu'))\n",
    "char_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "char_cnn.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "char_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f618448f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/135 [..............................] - ETA: 17s - loss: 0.6928 - accuracy: 0.5151 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 20:28:03.541576: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2024-04-16 20:28:03.541634: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2024-04-16 20:28:03.694377: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2024-04-16 20:28:03.695258: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2024-04-16 20:28:03.701160: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03\n",
      "\n",
      "2024-04-16 20:28:03.704520: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03/c05a32572f15.trace.json.gz\n",
      "2024-04-16 20:28:03.709339: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03\n",
      "\n",
      "2024-04-16 20:28:03.711518: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03/c05a32572f15.memory_profile.json.gz\n",
      "2024-04-16 20:28:03.726184: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03\n",
      "Dumped tool data for xplane.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03/c05a32572f15.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03/c05a32572f15.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03/c05a32572f15.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03/c05a32572f15.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_28_03/c05a32572f15.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 [==============================] - 20s 141ms/step - loss: 0.6705 - accuracy: 0.5876 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 2/20\n",
      "135/135 [==============================] - 19s 140ms/step - loss: 0.6435 - accuracy: 0.6194 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 3/20\n",
      "135/135 [==============================] - 20s 146ms/step - loss: 0.6146 - accuracy: 0.6546 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 4/20\n",
      "135/135 [==============================] - 20s 148ms/step - loss: 0.5846 - accuracy: 0.6836 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 5/20\n",
      "135/135 [==============================] - 20s 150ms/step - loss: 0.5610 - accuracy: 0.7042 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 6/20\n",
      "135/135 [==============================] - 21s 154ms/step - loss: 0.5426 - accuracy: 0.7198 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 7/20\n",
      "135/135 [==============================] - 19s 143ms/step - loss: 0.5273 - accuracy: 0.7315 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 8/20\n",
      "135/135 [==============================] - 19s 144ms/step - loss: 0.5117 - accuracy: 0.7438 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 9/20\n",
      "135/135 [==============================] - 20s 149ms/step - loss: 0.4993 - accuracy: 0.7516 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 10/20\n",
      "135/135 [==============================] - 19s 141ms/step - loss: 0.4874 - accuracy: 0.7599 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 11/20\n",
      "135/135 [==============================] - 19s 143ms/step - loss: 0.4752 - accuracy: 0.7686 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 12/20\n",
      "135/135 [==============================] - 21s 153ms/step - loss: 0.4622 - accuracy: 0.7759 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 13/20\n",
      "135/135 [==============================] - 19s 143ms/step - loss: 0.4498 - accuracy: 0.7844 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 14/20\n",
      "135/135 [==============================] - 19s 142ms/step - loss: 0.4415 - accuracy: 0.7903 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 15/20\n",
      "135/135 [==============================] - 20s 150ms/step - loss: 0.4296 - accuracy: 0.7975 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 16/20\n",
      "135/135 [==============================] - 21s 153ms/step - loss: 0.4192 - accuracy: 0.8027 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 17/20\n",
      "135/135 [==============================] - 20s 146ms/step - loss: 0.4063 - accuracy: 0.8108 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 18/20\n",
      "135/135 [==============================] - 20s 150ms/step - loss: 0.3935 - accuracy: 0.8186 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 19/20\n",
      "135/135 [==============================] - 23s 171ms/step - loss: 0.3873 - accuracy: 0.8221 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n",
      "Epoch 20/20\n",
      "135/135 [==============================] - 22s 164ms/step - loss: 0.3774 - accuracy: 0.8263 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff351a941c0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn.fit(\n",
    "    X_train_char_enc, y_train,\n",
    "    validation_data=[X_val_char_enc, y_val], \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback_1, callback_2, callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b3cb8428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.3 s, sys: 7.6 s, total: 42.9 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# lstm inference\n",
    "pred_proba_train_char_cnn = char_cnn.predict(X_train_char_enc)\n",
    "pred_proba_val_char_cnn = char_cnn.predict(X_val_char_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3240f57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (Train):  0.9321483904024039\n",
      "ROC-AUC (Validation):  0.7690857545502849\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75      6295\n",
      "           1       0.82      0.84      0.83      8966\n",
      "\n",
      "    accuracy                           0.79     15261\n",
      "   macro avg       0.79      0.79      0.79     15261\n",
      "weighted avg       0.79      0.79      0.79     15261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "pre_labels_val = np.where(pred_proba_val_char_cnn > threshold, 1, 0)\n",
    "\n",
    "roc_auc_train_char_cnn = roc_auc_score(y_train, pred_proba_train_char_cnn)\n",
    "roc_auc_val_char_cnn = roc_auc_score(y_val, pred_proba_val_char_cnn)\n",
    "class_report_char_cnn = classification_report(y_val, pre_labels_val)\n",
    "\n",
    "print('ROC-AUC (Train): ', roc_auc_train_char_cnn)\n",
    "print('ROC-AUC (Validation): ', roc_auc_val_char_cnn)\n",
    "\n",
    "print('\\n', class_report_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31ca17",
   "metadata": {},
   "source": [
    "**Results Analysis**\n",
    "- The main issue -> we don't consider word orders because there is no RNN inside\n",
    "\n",
    "### OHE Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7aafabbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE function\n",
    "def apply_ohe(x, sz):\n",
    "    return tf.cast(tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6e8827f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test OHE transformation\n",
    "# apply_ohe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "865f4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"char_cnn_ohe\"\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "# input initialization\n",
    "in_sentence = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int64')\n",
    "\n",
    "# Lambda layer for OHE transformation\n",
    "embedded = Lambda(\n",
    "    apply_ohe,\n",
    "    output_shape=lambda x: (x[0], x[1], vocab_size),\n",
    "    arguments={\"sz\": vocab_size}\n",
    ")(in_sentence)\n",
    "\n",
    "block = embedded\n",
    "\n",
    "# convolutions with MaxPooling\n",
    "for i in range(3):\n",
    "    block = Conv1D(activation=\"relu\", filters=100, kernel_size=4, padding=\"valid\")(block)\n",
    "    if i == 0:\n",
    "        block = MaxPooling1D(pool_size=5)(block)\n",
    "        \n",
    "# LSTM cell\n",
    "block = LSTM(128, dropout=0.1, recurrent_dropout=0.1)(block)\n",
    "block = Dense(100, activation='relu')(block)\n",
    "block = Dense(1, activation='sigmoid')(block)\n",
    "\n",
    "# build model\n",
    "char_cnn_ohe = Model(inputs=in_sentence, outputs=block)\n",
    "char_cnn_ohe.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2d20830c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  2/135 [..............................] - ETA: 10s - loss: 0.6916 - accuracy: 0.5151 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 20:48:05.041191: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2024-04-16 20:48:05.041269: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2024-04-16 20:48:05.172974: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2024-04-16 20:48:05.176036: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2024-04-16 20:48:05.184692: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05\n",
      "\n",
      "2024-04-16 20:48:05.189958: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05/c05a32572f15.trace.json.gz\n",
      "2024-04-16 20:48:05.199368: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05\n",
      "\n",
      "2024-04-16 20:48:05.201726: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05/c05a32572f15.memory_profile.json.gz\n",
      "2024-04-16 20:48:05.217782: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05\n",
      "Dumped tool data for xplane.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05/c05a32572f15.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05/c05a32572f15.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05/c05a32572f15.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05/c05a32572f15.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/logs_simple_rnn/train/plugins/profile/2024_04_16_20_48_05/c05a32572f15.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/135 [============================>.] - ETA: 0s - loss: 0.6666 - accuracy: 0.5935"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1330 test_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1267 test_step\n        y_pred = self(x, training=False)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py:199 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_1 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 40) dtype=int32>, <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/3055426086.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m char_cnn_ohe.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mX_train_char_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_val_char_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1215\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1216\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 759\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    760\u001b[0m             *args, **kwds))\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1330 test_function  *\n        return step_function(self, iterator)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1320 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1313 run_step  **\n        outputs = model.test_step(data)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1267 test_step\n        y_pred = self(x, training=False)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/input_spec.py:199 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer model_1 expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 40) dtype=int32>, <tf.Tensor 'ExpandDims:0' shape=(None, 1) dtype=int64>]\n"
     ]
    }
   ],
   "source": [
    "char_cnn_ohe.fit(\n",
    "    X_train_char_enc, y_train,\n",
    "    validation_data=[X_val_char_enc, y_val], \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback_1, callback_2, callback_3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc22fbdd",
   "metadata": {},
   "source": [
    "**CNN Advantages over LSTM**\n",
    "- No need to store thousands of words, only some number of symbols \n",
    "- Typos don't affect on model performance (e.g. `\"the bst film\"` -> will be well classified by CNN, LSTM will ignore)\n",
    "- Can be applied for any language\n",
    "- Doesn't require to have a big vocabularly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c15fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# lstm inference\n",
    "pred_proba_train_char_cnn = char_cnn_ohe.predict(X_train_char_enc)\n",
    "pred_proba_val_char_cnn = char_cnn_ohe.predict(X_val_char_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5f95258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC (Train):  0.6703052980671775\n",
      "ROC-AUC (Validation):  0.6636913929721618\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.73      0.75      6295\n",
      "           1       0.82      0.84      0.83      8966\n",
      "\n",
      "    accuracy                           0.79     15261\n",
      "   macro avg       0.79      0.79      0.79     15261\n",
      "weighted avg       0.79      0.79      0.79     15261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "pre_labels_val = np.where(pred_proba_val_char_cnn > threshold, 1, 0)\n",
    "\n",
    "roc_auc_train_char_cnn = roc_auc_score(y_train, pred_proba_train_char_cnn)\n",
    "roc_auc_val_char_cnn = roc_auc_score(y_val, pred_proba_val_char_cnn)\n",
    "class_report_char_cnn = classification_report(y_val, pre_labels_val)\n",
    "\n",
    "print('ROC-AUC (Train): ', roc_auc_train_char_cnn)\n",
    "print('ROC-AUC (Validation): ', roc_auc_val_char_cnn)\n",
    "\n",
    "print('\\n', class_report_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb9a61",
   "metadata": {},
   "source": [
    "### Best Practice\n",
    "\n",
    "**How else Performance Might be Improved**\n",
    "- Increase embeddings dimension\n",
    "- Increase LSTM output dimension\n",
    "- Add more layers and Bidirectional LSTM (when have enough data and overfitting doesn't happen)\n",
    "- Switching from little to bigger batch\n",
    "- Trying different dropout values and optimizaers\n",
    "- Increase vocab size (more symbols, words, ...)\n",
    "- Play around with `MAX_SEQUENCE_LENGTH`\n",
    "- Different Architectures\n",
    "\n",
    "**Advice from Practice**\n",
    "- little data and text len is short -> linear methods (e.g. LogReg or SVM with n-gramms with hard regularization)\n",
    "- little data and text lengthy -> LSTM with not many layers\n",
    "- much data -> different LSTM and CNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65df3957",
   "metadata": {},
   "source": [
    "### Reference\n",
    "- [GitNotebook](https://github.com/Yorko/mlcourse.ai/blob/main/jupyter_russian/tutorials/dl_sentiment_analysis_radchenko.ipynb)\n",
    "- [Nice Repo](https://github.com/udsclub/workshop/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f59a38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Themes: \r\n",
      "   chesterish\r\n",
      "   grade3\r\n",
      "   gruvboxd\r\n",
      "   gruvboxl\r\n",
      "   monokai\r\n",
      "   oceans16\r\n",
      "   onedork\r\n",
      "   solarizedd\r\n",
      "   solarizedl\r\n"
     ]
    }
   ],
   "source": [
    "!jt -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ce6c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t gruvboxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2bf3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
