{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catch Me If You Can: Intruder Detection through Webpage Session Tracking\n",
    "We try to identify a user on the Internet tracking his/her sequence of attended Web pages. The algorithm to be built will take a webpage session (a sequence of webpages attended consequently by the same person) and predict whether it belongs to Alice or somebody else.\n",
    "\n",
    "**Objects**\n",
    "- Websites visits of either Alice or Intruder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/jovyan/work/data_sets/alice_catch_me/sessions_train.csv')\n",
    "test_df = pd.read_csv('/home/jovyan/work/data_sets/alice_catch_me/sessions_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id  site1                time1  site2                time2  \\\n",
       "0           1    718  2014-02-20 10:02:45    NaN                  NaN   \n",
       "1           2    890  2014-02-22 11:19:50  941.0  2014-02-22 11:19:50   \n",
       "2           3  14769  2013-12-16 16:40:17   39.0  2013-12-16 16:40:18   \n",
       "3           4    782  2014-03-28 10:52:12  782.0  2014-03-28 10:52:42   \n",
       "4           5     22  2014-02-28 10:53:05  177.0  2014-02-28 10:55:22   \n",
       "\n",
       "     site3                time3    site4                time4  site5  ...  \\\n",
       "0      NaN                  NaN      NaN                  NaN    NaN  ...   \n",
       "1   3847.0  2014-02-22 11:19:51    941.0  2014-02-22 11:19:51  942.0  ...   \n",
       "2  14768.0  2013-12-16 16:40:19  14769.0  2013-12-16 16:40:19   37.0  ...   \n",
       "3    782.0  2014-03-28 10:53:12    782.0  2014-03-28 10:53:42  782.0  ...   \n",
       "4    175.0  2014-02-28 10:55:22    178.0  2014-02-28 10:55:23  177.0  ...   \n",
       "\n",
       "                 time6    site7                time7    site8  \\\n",
       "0                  NaN      NaN                  NaN      NaN   \n",
       "1  2014-02-22 11:19:51   3847.0  2014-02-22 11:19:52   3846.0   \n",
       "2  2013-12-16 16:40:19  14768.0  2013-12-16 16:40:20  14768.0   \n",
       "3  2014-03-28 10:54:42    782.0  2014-03-28 10:55:12    782.0   \n",
       "4  2014-02-28 10:55:59    175.0  2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                 time8    site9                time9   site10  \\\n",
       "0                  NaN      NaN                  NaN      NaN   \n",
       "1  2014-02-22 11:19:52   1516.0  2014-02-22 11:20:15   1518.0   \n",
       "2  2013-12-16 16:40:21  14768.0  2013-12-16 16:40:22  14768.0   \n",
       "3  2014-03-28 10:55:42    782.0  2014-03-28 10:56:12    782.0   \n",
       "4  2014-02-28 10:55:59    177.0  2014-02-28 10:57:06    178.0   \n",
       "\n",
       "                time10  target  \n",
       "0                  NaN       0  \n",
       "1  2014-02-22 11:20:16       0  \n",
       "2  2013-12-16 16:40:24       0  \n",
       "3  2014-03-28 10:56:42       0  \n",
       "4  2014-02-28 10:57:11       0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    251264\n",
       " 1      2297\n",
       " Name: target, dtype: int64,\n",
       " 0    99.094104\n",
       " 1     0.905896\n",
       " Name: target, dtype: float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution\n",
    "(\n",
    "    train_df['target'].value_counts(),\n",
    "    train_df['target'].value_counts(normalize=True)*100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- highly imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Unique Sessions:  253561\n",
      "N Unique Websites:  41610\n"
     ]
    }
   ],
   "source": [
    "# look at number of sessions and websites\n",
    "print('N Unique Sessions: ', train_df['session_id'].value_counts().sum())\n",
    "\n",
    "site_columns = train_df.columns[train_df.columns.str.contains('site')]\n",
    "time_columns = train_df.columns[train_df.columns.str.contains('time')]\n",
    "\n",
    "sites_vocab = set()\n",
    "\n",
    "for col in site_columns:\n",
    "    sites_vocab = sites_vocab.union(set(train_df[col].unique()))\n",
    "\n",
    "print('N Unique Websites: ', len(sites_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2013-01-12 08:05:57'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['time1'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time min: 2013-01-12 08:05:57, Time max: 2014-04-30 23:39:53 (Train)\n",
      "Time min: 2014-05-01 17:14:03, Time max: 2014-12-05 19:10:03 (Test)\n"
     ]
    }
   ],
   "source": [
    "# preprocess time column\n",
    "train_df[time_columns] = train_df[time_columns].apply(pd.to_datetime)\n",
    "test_df[time_columns] = test_df[time_columns].apply(pd.to_datetime)\n",
    "\n",
    "# what time period we have\n",
    "print(f'Time min: {train_df[\"time1\"].min()}, Time max: {train_df[\"time10\"].max()} (Train)')\n",
    "print(f'Time min: {test_df[\"time1\"].min()}, Time max: {test_df[\"time10\"].max()} (Test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "- Since we have sequences / sites that are visited in sessions and time, we have to consider time order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sort_values(by='time1', ascending=True)\n",
    "test_df = test_df.sort_values(by='time1', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN -> 0\n",
    "train_df[site_columns] = train_df[site_columns].fillna(0).astype('int').astype('str')\n",
    "test_df[site_columns] = test_df[site_columns].fillna(0).astype('int').astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21668</th>\n",
       "      <td>21669</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54842</th>\n",
       "      <td>54843</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77291</th>\n",
       "      <td>77292</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114020</th>\n",
       "      <td>114021</td>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146669</th>\n",
       "      <td>146670</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        session_id site1               time1 site2               time2 site3  \\\n",
       "21668        21669    56 2013-01-12 08:05:57    55 2013-01-12 08:05:57     0   \n",
       "54842        54843    56 2013-01-12 08:37:23    55 2013-01-12 08:37:23    56   \n",
       "77291        77292   946 2013-01-12 08:50:13   946 2013-01-12 08:50:14   951   \n",
       "114020      114021   945 2013-01-12 08:50:17   948 2013-01-12 08:50:17   949   \n",
       "146669      146670   947 2013-01-12 08:50:20   950 2013-01-12 08:50:20   948   \n",
       "\n",
       "                     time3 site4               time4 site5  ...  \\\n",
       "21668                  NaT     0                 NaT     0  ...   \n",
       "54842  2013-01-12 09:07:07    55 2013-01-12 09:07:09     0  ...   \n",
       "77291  2013-01-12 08:50:15   946 2013-01-12 08:50:15   946  ...   \n",
       "114020 2013-01-12 08:50:18   948 2013-01-12 08:50:18   945  ...   \n",
       "146669 2013-01-12 08:50:20   947 2013-01-12 08:50:21   950  ...   \n",
       "\n",
       "                     time6 site7               time7 site8  \\\n",
       "21668                  NaT     0                 NaT     0   \n",
       "54842                  NaT     0                 NaT     0   \n",
       "77291  2013-01-12 08:50:16   948 2013-01-12 08:50:16   784   \n",
       "114020 2013-01-12 08:50:18   947 2013-01-12 08:50:19   945   \n",
       "146669 2013-01-12 08:50:21   946 2013-01-12 08:50:21   951   \n",
       "\n",
       "                     time8 site9               time9 site10  \\\n",
       "21668                  NaT     0                 NaT      0   \n",
       "54842                  NaT     0                 NaT      0   \n",
       "77291  2013-01-12 08:50:16   949 2013-01-12 08:50:17    946   \n",
       "114020 2013-01-12 08:50:19   946 2013-01-12 08:50:19    946   \n",
       "146669 2013-01-12 08:50:22   946 2013-01-12 08:50:22    947   \n",
       "\n",
       "                    time10 target  \n",
       "21668                  NaT      0  \n",
       "54842                  NaT      0  \n",
       "77291  2013-01-12 08:50:17      0  \n",
       "114020 2013-01-12 08:50:20      0  \n",
       "146669 2013-01-12 08:50:22      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "Many different models and features can be built on this dataset. We will train Word2Vec model tolearn association/dependencies between the websites visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create texts to train word2vec\n",
    "train_df['site_seq'] = train_df['site1']\n",
    "test_df['site_seq'] = test_df['site1']\n",
    "\n",
    "for s in site_columns[1:]:\n",
    "    train_df['site_seq'] = train_df['site_seq'] + \",\" + train_df[s]\n",
    "    test_df['site_seq'] = test_df['site_seq'] + \",\" + test_df[s]\n",
    "\n",
    "train_df['site_seq_list'] = train_df['site_seq'].apply(lambda x: x.split(','))\n",
    "test_df['site_seq_list'] = test_df['site_seq'].apply(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21668                      [56, 55, 0, 0, 0, 0, 0, 0, 0, 0]\n",
       "54842                    [56, 55, 56, 55, 0, 0, 0, 0, 0, 0]\n",
       "77291     [946, 946, 951, 946, 946, 945, 948, 784, 949, ...\n",
       "114020    [945, 948, 949, 948, 945, 946, 947, 945, 946, ...\n",
       "146669    [947, 950, 948, 947, 950, 952, 946, 951, 946, ...\n",
       "                                ...                        \n",
       "12223            [50, 50, 48, 49, 48, 52, 52, 49, 303, 304]\n",
       "164437    [4207, 753, 753, 52, 50, 4207, 3346, 3359, 334...\n",
       "12220     [52, 3346, 784, 784, 3346, 979, 3324, 7330, 35...\n",
       "156967    [3328, 3324, 3599, 3413, 753, 3328, 3599, 3359...\n",
       "204761     [222, 3346, 3346, 3359, 55, 2891, 3346, 0, 0, 0]\n",
       "Name: site_seq_list, Length: 253561, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtained site seq for each session\n",
    "train_df['site_seq_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "- Sentence -> sequence of websites visited by a user\n",
    "- It's unnecessary transform numbers into website names because algorithm will learn correlation/dependencies anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for better word2vec model training concatenate train and test (use both train and test only for word2vec)\n",
    "test_df['target'] = -1\n",
    "data = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# define word2vec model\n",
    "w2v_model = word2vec.Word2Vec(data['site_seq_list'], vector_size=300, window=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocab and embeddigs\n",
    "embeddings_vocab = dict(zip(w2v_model.wv.index_to_key, w2v_model.wv.vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now each word has a vector but in our case a session consists of websites/words\n",
    "- We need to decide how can we represent a sentence using a set of word embeddings?\n",
    "- **One solution** -> **get average embedding vector** -> average meaning of a sentence\n",
    "\n",
    "### Mean Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom 'MeanVectorizer'\n",
    "class MeanSentenceVectorizer():\n",
    "    \"\"\"\n",
    "    Class to get mean embeddings of sentences.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings_vocab: Dict[str, np.ndarray]) -> None:\n",
    "        self.embeddings_vocab = embeddings_vocab # word name and its embedding\n",
    "        self.dim = len(next(iter(self.embeddings_vocab.values()))) # embedding dimension\n",
    "\n",
    "\n",
    "    def transform(self, X: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gets mean embedding of a sentence. If token is missing in model vocab,\n",
    "        returns zero vector.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: str\n",
    "            Sentence (i.e. sequence of tokens).\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            np.mean(\n",
    "                [self.embeddings_vocab[w] for w in words if w in self.embeddings_vocab] \n",
    "                or\n",
    "                [np.zeros(self.dim)], axis=0\n",
    "            )\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained Embeddings Shape:  (253561, 300)\n"
     ]
    }
   ],
   "source": [
    "# get mean embeddings of sentences / sessions\n",
    "mean_sen_vec = MeanSentenceVectorizer(embeddings_vocab)\n",
    "\n",
    "# get transformations for both train and test\n",
    "mean_embeddings_train = mean_sen_vec.transform(train_df['site_seq_list'])\n",
    "mean_embeddings_test = mean_sen_vec.transform(test_df['site_seq_list'])\n",
    "\n",
    "train_df['seq_mean_embedding'] = list(mean_embeddings_train)\n",
    "test_df['seq_mean_embedding'] = list(mean_embeddings_test)\n",
    "\n",
    "print('Obtained Embeddings Shape: ', mean_embeddings_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have our embeddings and can train a model\n",
    "- Apply train test split first to get validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    99.027351\n",
       " 1     0.972649\n",
       " Name: target, dtype: float64,\n",
       " 0    99.361111\n",
       " 1     0.638889\n",
       " Name: target, dtype: float64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't shuffle since time is important! (first we always check the performance and train and validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    mean_embeddings_train, train_df['target'],\n",
    "    train_size=0.8, shuffle=False, random_state=23\n",
    ")\n",
    "\n",
    "# to check the ratio of samples in train and validation\n",
    "(\n",
    "    y_train.value_counts(normalize=True)*100,\n",
    "    y_val.value_counts(normalize=True)*100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "**Simple Neural Net**\n",
    "\n",
    "- To predict if a session is fraudulent, train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.layers import Activation, Dense, Dropout, Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=(X_train.shape[1])))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['binary_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1585/1585 [==============================] - 5s 3ms/step - loss: 0.0490 - binary_accuracy: 0.9896 - val_loss: 0.0310 - val_binary_accuracy: 0.9936\n",
      "Epoch 2/10\n",
      "1585/1585 [==============================] - 4s 2ms/step - loss: 0.0433 - binary_accuracy: 0.9903 - val_loss: 0.0304 - val_binary_accuracy: 0.9937\n",
      "Epoch 3/10\n",
      "1585/1585 [==============================] - 3s 2ms/step - loss: 0.0420 - binary_accuracy: 0.9904 - val_loss: 0.0292 - val_binary_accuracy: 0.9936\n",
      "Epoch 4/10\n",
      "1585/1585 [==============================] - 3s 2ms/step - loss: 0.0408 - binary_accuracy: 0.9905 - val_loss: 0.0290 - val_binary_accuracy: 0.9937\n",
      "Epoch 5/10\n",
      "1585/1585 [==============================] - 4s 2ms/step - loss: 0.0404 - binary_accuracy: 0.9905 - val_loss: 0.0284 - val_binary_accuracy: 0.9937\n",
      "Epoch 6/10\n",
      "1585/1585 [==============================] - 4s 2ms/step - loss: 0.0399 - binary_accuracy: 0.9906 - val_loss: 0.0283 - val_binary_accuracy: 0.9937\n",
      "Epoch 7/10\n",
      "1585/1585 [==============================] - 4s 3ms/step - loss: 0.0393 - binary_accuracy: 0.9906 - val_loss: 0.0276 - val_binary_accuracy: 0.9936\n",
      "Epoch 8/10\n",
      "1585/1585 [==============================] - 3s 2ms/step - loss: 0.0391 - binary_accuracy: 0.9906 - val_loss: 0.0280 - val_binary_accuracy: 0.9937\n",
      "Epoch 9/10\n",
      "1585/1585 [==============================] - 4s 2ms/step - loss: 0.0386 - binary_accuracy: 0.9906 - val_loss: 0.0277 - val_binary_accuracy: 0.9937\n",
      "Epoch 10/10\n",
      "1585/1585 [==============================] - 4s 2ms/step - loss: 0.0383 - binary_accuracy: 0.9906 - val_loss: 0.0274 - val_binary_accuracy: 0.9937\n"
     ]
    }
   ],
   "source": [
    "# train a model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train, \n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val),\n",
    "#     class_weight='auto',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9236614448234709"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get performance (validation set)\n",
    "y_pred_nn_val = model.predict(X_val, batch_size=128)\n",
    "roc_auc_score(y_val, y_pred_nn_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test model performance on Leaderboard (Test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_submission(ids, preds, f_name=''):\n",
    "    submission_df = pd.DataFrame({\n",
    "        'session_id': ids,\n",
    "        'target': preds\n",
    "    })\n",
    "    submission_df.to_csv(f_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "y_pred_nn_test = model.predict(mean_embeddings_test, batch_size=128)\n",
    "\n",
    "prediction_to_submission(\n",
    "    test_df['session_id'], np.squeeze(y_pred_nn_test).tolist(), 'mean_embeddings_nn.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "- The performance of the model is close to a baseline (log_reg + OHE(sites)). However, the dimensionality is much less, thus, word2vec model was able to identify some dependencies among sessions.\n",
    "- Performance on Leaderboard (Test): `0.89974`\n",
    "\n",
    "**Tree-Based Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for xgb\n",
    "train_xgb_matrix = xgb.DMatrix(X_train, label=y_train, missing = np.nan)\n",
    "val_xgb_matrix = xgb.DMatrix(X_val, label=y_val, missing = np.nan)\n",
    "test_xgb_matrix = xgb.DMatrix(mean_embeddings_test)\n",
    "\n",
    "watchlist = [(train_xgb_matrix, 'train'), (val_xgb_matrix, 'eval')]\n",
    "history = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.95504\teval-auc:0.85363\n",
      "[20]\ttrain-auc:0.98877\teval-auc:0.91651\n",
      "[40]\ttrain-auc:0.99146\teval-auc:0.91991\n",
      "[60]\ttrain-auc:0.99308\teval-auc:0.92145\n",
      "[80]\ttrain-auc:0.99425\teval-auc:0.92304\n",
      "[99]\ttrain-auc:0.99511\teval-auc:0.92383\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'max_depth': 26,\n",
    "    'eta': 0.025,\n",
    "    'nthread': 4,\n",
    "    'gamma' : 1,\n",
    "    'alpha' : 1,\n",
    "    'subsample': 0.85,\n",
    "    'eval_metric': ['auc'],\n",
    "    'objective': 'binary:logistic',\n",
    "    'colsample_bytree': 0.9,\n",
    "    'min_child_weight': 100,\n",
    "    'scale_pos_weight': (1)/train_df['target'].mean(),\n",
    "    'seed': 23\n",
    "}\n",
    "\n",
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    train_xgb_matrix,\n",
    "    num_boost_round=100,\n",
    "    evals=watchlist,\n",
    "    evals_result=history,\n",
    "    verbose_eval=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9238325825080871"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance estimation (validation data)\n",
    "scores_val = xgb_model.predict(val_xgb_matrix)\n",
    "roc_auc_score(y_val, scores_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb_test = xgb_model.predict(test_xgb_matrix)\n",
    "\n",
    "prediction_to_submission(\n",
    "    test_df['session_id'], np.squeeze(y_pred_xgb_test).tolist(), 'mean_embeddings_xgb.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Analysis**\n",
    "- XGBoost overfits the training data and thus val performance is not good, let's check linear models\n",
    "- Performance on Leaderboard (Test): `0.89870` although val performance is a bit better\n",
    "\n",
    "**Linear Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, n_jobs=-1, random_state=23)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C': 1,\n",
    "    'random_state': 23,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "mean_lr_model = LogisticRegression(**params)\n",
    "mean_lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9009595164435507"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance estimation\n",
    "y_pred_lr_mean = mean_lr_model.predict_proba(X_val)[:, 1]\n",
    "roc_auc_score(y_val, y_pred_lr_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF Weighted Mean Embeddings\n",
    "- Let's assign a weight in front of every word in a text and **compute weighted average**\n",
    "- Weight -> `idf`\n",
    "    - High weight for words that rare -> Can better distinguish between similar documents\n",
    "    - Low weight for words that are common and appear in many documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTfIdfVectorizer():\n",
    "    def __init__(self, embeddings_vocab: Dict[str, np.ndarray]) -> None:\n",
    "        self.embeddings_vocab = embeddings_vocab\n",
    "        self.dim = len(next(iter(self.embeddings_vocab.values())))\n",
    "        self.weight = None\n",
    "\n",
    "\n",
    "    def fit(self, X: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Computes IDF weights for tokens.\n",
    "        \n",
    "        Important\n",
    "        ---------\n",
    "        Words that are missed in vocab must be as infrequent as any known words.\n",
    "        Thus, the default IDF value for them is max(idf).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list\n",
    "            Sentence (i.e. sequence of tokens).\n",
    "        \"\"\"\n",
    "        # fit TfidfVectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf_vectorizer.fit(X)\n",
    "\n",
    "        max_idf = max(tfidf_vectorizer.idf_) # this weight will have words that are missing in vocab\n",
    "\n",
    "        self.weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [\n",
    "                (w, tfidf_vectorizer.idf_[i]) for w, i in tfidf_vectorizer.vocabulary_.items()\n",
    "            ]\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    def transform(self, X: List[str]) -> np.ndarray:\n",
    "        \"\"\"Returns IDF weighted mean embedding vector of a sentence.\"\"\"\n",
    "        return np.array([\n",
    "            np.mean(\n",
    "                [self.embeddings_vocab[w] * self.weight[w] for w in words if w in self.embeddings_vocab] \n",
    "                or\n",
    "                [np.zeros(self.dim)], axis=0 # if word is missing, it has zero vector and max(idf)\n",
    "            )\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained Embeddings Shape:  (253561, 300)\n"
     ]
    }
   ],
   "source": [
    "# get idf weighted mean embeddings of sentences / sessions\n",
    "idf_mean_sen_vec = CustomTfIdfVectorizer(embeddings_vocab)\n",
    "idf_mean_sen_vec.fit(train_df['site_seq_list'])\n",
    "\n",
    "idf_mean_embeddings = idf_mean_sen_vec.transform(train_df['site_seq_list'])\n",
    "train_df['seq_idf_mean_embedding'] = list(idf_mean_embeddings)\n",
    "\n",
    "print('Obtained Embeddings Shape: ', idf_mean_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check Improvement**\n",
    "- Use logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    idf_mean_embeddings, train_df['target'],\n",
    "    train_size=0.8, shuffle=False, random_state=23\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, n_jobs=-1, random_state=23)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "params = {\n",
    "    'C': 1,\n",
    "    'random_state': 23,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "lr_model = LogisticRegression(**params)\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8977889366408356"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performance estimation\n",
    "y_pred_lr = lr_model.predict_proba(X_val)[:, 1]\n",
    "score_lr = roc_auc_score(y_val, y_pred_lr)\n",
    "score_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Analysis**\n",
    "\n",
    "There is a small improvement that supports the usefulness of weighting the word embeddings:\n",
    "- simple embeddings average: `0.8922`\n",
    "- weighted embeddings average: `0.8989`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaderboard Performance\n",
    "Let's check how well we trained our model.\n",
    "- [Competition Leaderboard](https://www.kaggle.com/competitions/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/leaderboard?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_mean_sen_vec = CustomTfIdfVectorizer(embeddings_vocab)\n",
    "mean_sen_vec = MeanSentenceVectorizer(embeddings_vocab)\n",
    "\n",
    "mean_embeddings_train = mean_sen_vec.transform(train_df['site_seq_list'])\n",
    "mean_embeddings_test = mean_sen_vec.transform(test_df['site_seq_list'])\n",
    "\n",
    "idf_mean_sen_vec.fit(train_df['site_seq_list'])\n",
    "\n",
    "idf_mean_embeddings_train = idf_mean_sen_vec.transform(train_df['site_seq_list'])\n",
    "idf_mean_embeddings_test = idf_mean_sen_vec.transform(test_df['site_seq_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, n_jobs=-1, random_state=23)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# use all data and train model\n",
    "params = {\n",
    "    'C': 1,\n",
    "    'random_state': 23,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "lr_model_mean = LogisticRegression(**params)\n",
    "lr_model_mean.fit(mean_embeddings_train, train_df['target'])\n",
    "\n",
    "lr_model_mean_idf = LogisticRegression(**params)\n",
    "lr_model_mean_idf.fit(idf_mean_embeddings_train, train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "pred_mean = lr_model_mean.predict_proba(mean_embeddings_test)[:, 1]\n",
    "pred_mean_idf = lr_model_mean_idf.predict_proba(idf_mean_embeddings_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_to_submission(\n",
    "    test_df['session_id'], pred_mean, 'mean_embeddings.csv'\n",
    ")\n",
    "\n",
    "prediction_to_submission(\n",
    "    test_df['session_id'], pred_mean_idf, 'idf_mean_embeddings.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mean_embeddings: `0.89886`\n",
    "- idf_mean_embeddings: `0.89501`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1981/1981 [==============================] - 4s 2ms/step - loss: 0.0347 - binary_accuracy: 0.9913\n",
      "Epoch 2/10\n",
      "1981/1981 [==============================] - 4s 2ms/step - loss: 0.0350 - binary_accuracy: 0.9913\n",
      "Epoch 3/10\n",
      "1981/1981 [==============================] - 4s 2ms/step - loss: 0.0346 - binary_accuracy: 0.9913\n",
      "Epoch 4/10\n",
      "1981/1981 [==============================] - 5s 2ms/step - loss: 0.0346 - binary_accuracy: 0.9914\n",
      "Epoch 5/10\n",
      "1981/1981 [==============================] - 4s 2ms/step - loss: 0.0345 - binary_accuracy: 0.9913\n",
      "Epoch 6/10\n",
      "1981/1981 [==============================] - 4s 2ms/step - loss: 0.0344 - binary_accuracy: 0.9913\n",
      "Epoch 7/10\n",
      "1981/1981 [==============================] - 4s 2ms/step - loss: 0.0341 - binary_accuracy: 0.9913\n",
      "Epoch 8/10\n",
      "1981/1981 [==============================] - 4s 2ms/step - loss: 0.0340 - binary_accuracy: 0.9914\n",
      "Epoch 9/10\n",
      "1981/1981 [==============================] - 4s 2ms/step - loss: 0.0341 - binary_accuracy: 0.9914\n",
      "Epoch 10/10\n",
      "1981/1981 [==============================] - 5s 3ms/step - loss: 0.0337 - binary_accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "# train a model\n",
    "history = model.fit(\n",
    "    idf_mean_embeddings_train,\n",
    "    train_df['target'], \n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "y_pred_nn_idf_mean_test = model.predict(mean_embeddings_test, batch_size=128)\n",
    "\n",
    "prediction_to_submission(\n",
    "    test_df['session_id'], np.squeeze(y_pred_nn_idf_mean_test).tolist(), 'idf_mean_embeddings_nn.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Performance on Leaderboard (Test): `0.90333` -> the best model\n",
    "\n",
    "### Further Potential\n",
    "We trained a model only on embeddings that allowed drastically reducing dimensionality. However, we have only embedding features and other features can be created as well. For example:\n",
    "- Time related features:\n",
    "    - Month, day, time of day, ...\n",
    "    - Time between sessions\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
