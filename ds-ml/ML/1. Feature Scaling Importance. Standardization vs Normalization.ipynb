{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling. Why is it important?\n",
    "Based on this article https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It improves (significantly) the performance of machine learning algorithms which are based on distances. That's why it is important to know when to apply it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Based Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models such as:\n",
    "- **Linear Regression**\n",
    "- **Logistic Regression**\n",
    "- **Neural Nets**\n",
    "\n",
    "Are based on Gradient Descent optimization algorithm, thus, scale of feature x affects on step size of gradient descent. The difference in ranges of features will cause different step sizes for each feature. To ensure that the gradient descent moves smoothly towards the minima and that the steps for gradient descent are updated at the same rate for all the features, we scale the data before feeding it to the model. **Having features on a similar scale can help the gradient descent converge more quickly towards the minima**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Based Algorithms \n",
    "Models such as:\n",
    "- **k - NN**\n",
    "- **k - means**\n",
    "- **SVM** \n",
    "\n",
    "are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity. **Therefore, we scale our data before employing a distance based algorithm so that all the features contribute equally to the result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Based Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are fairly **insensitive to the scale of the features**. Think about it, a decision tree is only splitting a node based on a single feature. The decision tree splits a node on a feature that increases the homogeneity of the node. **This split on a feature is not influenced by other features**.\n",
    "So, there is virtually no effect of the remaining features on the split. This is what makes them invariant to the scale of the features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods\n",
    "- **PCA** - Does require ( Standardization is preferable ) \n",
    "- **LDA** - Does require\n",
    "- **Naive Bayes** - Does not reqire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a scaling technique in which values are rescaled so that they end up ranging between 0 and 1.\n",
    "\n",
    "**It is also known as Min-Max scaling**.\n",
    "\n",
    "When **X = Xmin** all equation turns into 0 and vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Big Question â€“ Normalize or Standardize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Normalization** is good to use when you know that the **data distribution does not follow a Gaussian distribution**. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n",
    "- **Standardization** can be helpful in cases where the **data follows a Gaussian distribution**. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n",
    "\n",
    "The choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data. You can always start by fitting your model to raw, normalized and standardized data and compare the performance for best results.\n",
    "\n",
    "### Important\n",
    "- Don't apply scaling to binary variables because there is no much sense in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
