{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In high dimensional dataset there are features that entirely irrelevant, insignificant and not important at all. Their contribution towards predicting can be either small or zero. As a result, the following problems are arising:\n",
    "- Unnecessary resourse allocation for useless features\n",
    "- Contribute noise for which a ML model can not perform good\n",
    "- Training Time is arising \n",
    "\n",
    "### Main Objectives of Feature Selection\n",
    "- Improving Predicting Performance\n",
    "- Reduce Training Time\n",
    "- Better Understanding of the Process \n",
    "\n",
    "### Diminsionality Reduction and Feature Selection. Are they the same?\n",
    "It is common misconception and these two methods are different. Both methods tend to **reduce the number of attributes** in the dataset, but a **dimensionality reduction** method does so by **creating new combinations of attributes** (sometimes known as feature transformation), whereas **feature selection** methods **include and exclude attributes present in the data without changing them**.\n",
    "\n",
    "### Dimensionality Reduction Methods \n",
    "- Principal Component Analysis\n",
    "- Linear Descriminant Analysis\n",
    "- Singular Value Decomposition\n",
    "- t - SNE\n",
    "\n",
    "### Feature Selection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters Methods (Univariate Analysis)\n",
    "\n",
    "Filter methods are generally used as a data preprocessing step and based on statistical methods. Each feature is being considered independently and ranked according to its significance values (usually correaltion with target variable).\n",
    "Examples of filter methods include: \n",
    "- Information Gain\n",
    "- Chi - Squared test \n",
    "- minimum Redundancy Maximum Relevance (mRmR)\n",
    "\n",
    "**Advantages**\n",
    "- Not computationally intensive\n",
    "- Faster than Embedded and Frapper methods\n",
    "- Good works when number of samples is less than number of features\n",
    "\n",
    "**Disadvantages**\n",
    "- Each feature is considered independantly,thus collective feature influence on target variable is impossible to detect\n",
    "\n",
    "### Wrapper Methods \n",
    "A model is being trained on different set of features. Two main approaches exist in this method:\n",
    "- Forward Selection ( from emty feature set till the set with the best features )\n",
    "- Backward Selection ( from full feature set till the set with the best features, RFE method is a good example)\n",
    "\n",
    "**Disadvantages**\n",
    "- Computationally intensive\n",
    "- If number of samples is less than number of features, a danger of **overfitting** is increasing\n",
    "\n",
    "### Embedded Methods \n",
    "It is just regularization methods where the main idea of not only minimizing the residuals but also use as little features as possible ( feature minimization ).\n",
    " - Ridge Regression\n",
    " - LASSO Regression\n",
    " - Elastic Net Regression\n",
    " \n",
    "### Conclusion\n",
    "Probably, Wrapper methods are the best, but they are computationally intesive and sometimes it is not possible to use them. In case of large dataset is is better to use Embedded Methods ( Ridgge, Elastic Net ). Filters methods are fast but not precise.\n",
    "My rate:\n",
    "1) Wrapper Methods\n",
    "2) Emdedded Methods (Regularization)\n",
    "3) Filter Methods\n",
    "\n",
    "### Useful Links\n",
    "https://habr.com/ru/post/264915/ (rus option )\n",
    "\n",
    "https://www.youtube.com/watch?v=ipb2MhSRGdw&feature=youtu.be (about regularization)\n",
    "\n",
    "https://towardsdatascience.com/feature-selection-techniques-for-classification-and-python-tips-for-their-application-10c0ddd7918b (feature selection)\n",
    "\n",
    "https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15 (RFCV + nice charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for feature selection\n",
    "\n",
    "# RFECV - stands for Recursive Feature Elimination with Cross Validation, RFE is the same but without \n",
    "from sklearn.feature_selection import chi2, RFE, RFECV\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
