{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Tutorial\n",
    "Web Scraping is a method for obtaining the data from the Internet by parsing certain web pages. However, not all web pages can be easily parsed (e.g. ajax, js elements and other elements that provides dynamic page interaction)\n",
    "\n",
    "### Tools For Scraping\n",
    "- **BeautifulSoup** and **Requests** (ordinary web pages)\n",
    "- **Scrapy** (ordinary web pages, better than BeautifulSoup)\n",
    "- **Selenium** (emitates browser, must be used when we need to emitate user behaviour)\n",
    "- **Phantom JS** (headless browsers, emitates user behaviour and fast)\n",
    "\n",
    "### Beautiful Soup\n",
    "Beautiful Soup allows parse a html page (i.e. it takes in an html pagea as an argument). Normally, it's used along with requests library (make requests to a server using an url).\n",
    "\n",
    "**Important**\n",
    "- BS4 methods return BS4 objects, thus methods such as ```find()``` and ```find_all()``` can be applied again\n",
    "- A dictionary with html attributes and their values can be passed to ```find()``` or ```find_all()```\n",
    "- ```find()``` and ```find_all()``` start searching elements from up to bottom\n",
    "- To find tags that don't have any attributes, used ```find_parent()``` or ```find_parents()```\n",
    "- ```next_sibling()``` and ```previous_sibling()``` - returns next/previous elements after a found tag \n",
    "- To get tag's attribute values use ```get('attribute_name')```\n",
    "\n",
    "### Selenium \n",
    "Selenium is a tool for browser action automation. It's commonly used for web applications testing and not only. It's a great tool for imitation user behaviour on a website as well for web scraping \n",
    "\n",
    "**Some methods**\n",
    "- ```driver.get('url')``` - opens a page\n",
    "- ```driver.close()``` - closes the current browser's tab\n",
    "- ```driver.quit()``` - closes the entire brwoser\n",
    "- ```driver.title``` - shows the page's title\n",
    "- ```driver.find_element_by_name('name')``` - finds an element by name (other options are available as well)\n",
    "- ```driver.page_source``` - returns an HTML of a current page (thus, scraping is possible)\n",
    "- ```driver.execute_script('return document.documentElement.outerHTML')``` - returns an HTML page\n",
    "\n",
    "\n",
    "**Important**\n",
    "- You can't use ```find_element_by_class_name``` if class name contains spaces\n",
    "- To find elements that don't have unique ids or class_names use **xpath syntax**\n",
    "\n",
    "Useful Links: https://habr.com/ru/post/250975/\n",
    "\n",
    "### Helenium\n",
    "Helenium is a modification of Selenium that allows using Selenium in a more convenient way as well as running a browser in a headless mode. All the info can be foudn here: https://github.com/mherrmann/selenium-python-helium\n",
    "- https://github.com/mherrmann/selenium-python-helium/blob/master/docs/cheatsheet.md\n",
    "\n",
    "### Phantom JS\n",
    "Also a headless browser\n",
    "- https://pythonspot.com/selenium-phantomjs/\n",
    "\n",
    "### Requests_Html \n",
    "A great option to scrape dynamic web pages. \n",
    "Links:\n",
    "- https://www.youtube.com/watch?v=0hiGp3lF6ig&list=PLRzwgpycm-FgQ9lP_JTfrCa9O573XiJph&index=4\n",
    "- https://pypi.org/project/requests-html/\n",
    "\n",
    "\n",
    "### Splash\n",
    "A tool forscraping dynamic pages. More info: https://www.youtube.com/watch?v=8q2K41QC2nQ\n",
    "\n",
    "### Scrapy\n",
    "Scrapy is a framework for web scraping. More info: https://www.youtube.com/watch?v=s4jtkzHhLzY\n",
    "\n",
    "### Scraping Tips\n",
    "1. The most secure way of accessing tags is using the following order (id, class, name)\n",
    "2. Use Fake-User-Agent\n",
    "3. Set timeout for Requests: ```time.sleep(random.uniform(1, 6))```\n",
    "4. Use proxy, VPN or TOR (https://www.youtube.com/watch?v=vJwcW2gCCE4)\n",
    "5. Avoid traps for scrapers (e.g. hidden links, IP can be blocked)\n",
    "6. Check if a publick API exists\n",
    "7. Concurrent for fast data retrieving (https://www.youtube.com/watch?v=aA6-ezS5dyY)\n",
    "8. Create a VENV for frameworks (e.g. Scrapy)\n",
    "9. Sometimes find_all can't find elements, use select or select_one instead\n",
    "10. To iterate over pages check what a site shows if a page that doesn't exist is called\n",
    "11. Always check the content of a page using a incognito mode \n",
    "12. When logging in, check not only XHR but Doc as well\n",
    "13. Always check the form, because there might be some sripts that change data when logging in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VENV Creation\n",
    "1. Be in a project folder\n",
    "2. ```python -m venv venv_name```\n",
    "3. Activate it (Linux: ```venv_name/bin/activate``` Windows: ```venv_name\\Scripts\\ativate.bat```)\n",
    "\n",
    "To deactivate venv use: ```deactivate```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
