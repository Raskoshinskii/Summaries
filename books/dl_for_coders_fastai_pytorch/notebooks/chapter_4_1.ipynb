{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5697cbb9-e09d-42b8-bae6-765f3f4b028b",
   "metadata": {},
   "source": [
    "### MNIST Digit Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ee16c4a-727c-4bb5-9999-0fbb8aed43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75d4f77-7d4b-459c-ad5c-a57a5375ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs location \n",
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae5fb1a2-5613-459b-9eca-a00f78e3d0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3s Train Shape:  torch.Size([6131, 28, 28])\n",
      "7s Train Shape:  torch.Size([6265, 28, 28])\n",
      "3s Validation Shape:  torch.Size([1010, 28, 28])\n",
      "7s Validation Shape:  torch.Size([1028, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Download train/validation data\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "\n",
    "threes_train = torch.stack([\n",
    "    tensor(Image.open(number)) for number in (path/'train'/'3').ls().sorted()\n",
    "])\n",
    "\n",
    "sevens_train = torch.stack([\n",
    "    tensor(Image.open(number)) for number in (path/'train'/'7').ls().sorted()\n",
    "])\n",
    "\n",
    "\n",
    "threes_val = torch.stack([\n",
    "    tensor(Image.open(number)) for number in (path/'valid'/'3').ls().sorted()\n",
    "])\n",
    "\n",
    "sevens_val = torch.stack([\n",
    "    tensor(Image.open(number)) for number in (path/'valid'/'7').ls().sorted()\n",
    "])\n",
    "\n",
    "print('3s Train Shape: ', threes_train.shape)\n",
    "print('7s Train Shape: ', sevens_train.shape)\n",
    "\n",
    "print('3s Validation Shape: ', threes_val.shape)\n",
    "print('7s Validation Shape: ', sevens_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae0d68-7333-416e-821e-6151bba53720",
   "metadata": {},
   "source": [
    "- **Binary classification tasks, classes are balanced**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557cff7f-0950-4bf8-997c-85de57930ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver to float and scale the imgs (0 - 1)\n",
    "threes_train = threes_train.float()/255\n",
    "sevens_train = sevens_train.float()/255\n",
    "\n",
    "threes_val = threes_val.float()/255\n",
    "sevens_val = sevens_val.float()/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2096cd5-cabd-4e89-8683-a619931e2e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tensor Shape:  torch.Size([12396, 784])\n",
      "Validation Tensor Shape:  torch.Size([2038, 784])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all imgs into a single tensor and change them from a list of matrices to a list of vectors\n",
    "train_x = torch.cat([threes_train, sevens_train]).view(-1, 28*28)\n",
    "val_x = torch.cat([threes_val, sevens_val]).view(-1, 28*28)\n",
    "\n",
    "print('Train Tensor Shape: ', train_x.shape)\n",
    "print('Validation Tensor Shape: ', val_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf66865a-b0e4-4280-9de4-544c0a333743",
   "metadata": {},
   "source": [
    "- `view` is PyTorch method that changes the shape of a tensor without changing its contents\n",
    "- `-1` means make this axis as big as necessary to fit all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c98f88af-834f-4e08-8121-c957d8b252f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we have only two classes (3s and 7s) - make labels for them as 1 and 0\n",
    "train_y = tensor([1]*threes_train.shape[0] + [0]*sevens_train.shape[0]).unsqueeze(1)\n",
    "val_y = tensor([1]*threes_val.shape[0] + [0]*sevens_val.shape[0]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a8c8f3-7a91-4bf1-9c93-03ffb6d61e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12396, 784]) torch.Size([12396, 1])\n",
      "torch.Size([2038, 784]) torch.Size([2038, 1])\n"
     ]
    }
   ],
   "source": [
    "# Get the shape \n",
    "print(train_x.shape, train_y.shape)\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "367a33bc-a6f6-4c51-8a1e-5caf1139b1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Dataset in PyTorch is required to return a tuple of (x, y)\n",
    "dset_train = list(zip(train_x, train_y))\n",
    "dset_val = list(zip(val_x, val_y))\n",
    "\n",
    "x, y = dset_train[0] # first observation and its label from train data\n",
    "x.shape, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b2ae8a-e928-4e0b-b3b5-f389e216242e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Shape:  torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "# Create a function for weight initialization\n",
    "def init_params(size, std=1.0):\n",
    "    return (torch.randn(size)*std).requires_grad_()\n",
    "\n",
    "weights = init_params(28*28, 1)\n",
    "print('Weights Shape: ', weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d3d28-30d3-487a-8af6-b7119bf692c3",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "The function `weights*pixels` won’t be flexible enough. It is always equal to 0 when the pixels are equal to 0. You might remember from high school\n",
    "math that the formula for a line is `y=w*x+b`. **We still need the b (bias).** We’ll initialize it to a random number too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98e0f3e8-96f2-491f-9b3a-357af369c993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4393], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = init_params(1)\n",
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d79bfaa-4c2c-43b8-955d-418032a6294b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.9566], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction for a single img\n",
    "pred = (train_x[0]*weights.T).sum() + bias\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b24b5-86a7-40a4-a866-a7c7ebc71f50",
   "metadata": {},
   "source": [
    "### How to Calculate Predictions for Each Image?\n",
    "- `Loop Usage` - very slow \n",
    "- `Matrix Multiplication` - very fast \n",
    "\n",
    "`train_x` is a matrix, `w` is a column-vector (matrix),  `b` is a vector. **Using all these we can get prediction for each image incredibly fast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5efdb5d-bece-4d00-bc7e-dde8262b52bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.9566, 12.0457, 12.6466,  ..., 20.3852,  7.5482, -3.0783],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_predictions(x, w, b):\n",
    "    return x@w + b # fundamental equation of any DL net\n",
    "\n",
    "preds = get_predictions(\n",
    "    train_x,\n",
    "    weights,\n",
    "    bias\n",
    ")\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a069037-9a8a-4d09-8ef5-b17d400c57ee",
   "metadata": {},
   "source": [
    "### How to Get the Labels?\n",
    "We have a binary classification problem. So we can say that if the prediction is greater than 0 - it's 3, otherwise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f780409-9d56-4b4b-8add-361c9a3b6c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.49598419666290283\n"
     ]
    }
   ],
   "source": [
    "is_correct_pred = (preds > 0.0).float() == train_y\n",
    "print('Accuracy: ', is_correct_pred.float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108f764-b50b-43d6-a503-8564265037ba",
   "metadata": {},
   "source": [
    "**Above we got this accuracy using random weights**\n",
    "\n",
    "Now let’s see what the change in accuracy is for a small change in one of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "241c5f15-6b31-4a63-b25c-401112a872a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49598419666290283"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0].data *= 1.0001\n",
    "\n",
    "preds = get_predictions(\n",
    "    train_x,\n",
    "    weights,\n",
    "    bias\n",
    ")\n",
    "\n",
    "((preds>0.0).float() == train_y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ba3e0-b1c6-452e-bb23-c008505c8917",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "To improve the model quality **we need to define the loss function and then get the gradients with respect to the weights.** \n",
    "\n",
    "### Loss Function Choosing\n",
    "\n",
    "So, we need to choose a loss function. The obvious approach would be to use accuracy, which is our metric, as our loss function as well. In this case, we would calculate our prediction for each image, collect these values to calculate an overall accuracy, and then calculate the gradients of each weight with respect to that overall accuracy.\n",
    "\n",
    "Unfortunately, we have a significant technical problem here. \n",
    "- **The gradient of a function is its slope, or its steepness**, \n",
    "\n",
    "It can be defined as rise over run—that is: *how much the value of the function goes up or down, divided by how much we changed\n",
    "the input.*\n",
    "\n",
    "We can write this mathematically as: \n",
    "- `(y_new – y_old) / (x_new – x_old)`\n",
    "\n",
    "This gives a good approximation of the gradient when `x_new` is very similar to `x_old`, meaning that their difference is very small. **But accuracy changes at all only when a prediction changes from a 3 to a 7, or vice versa.**\n",
    "\n",
    "- The problem is that a small change in weights from `x_old` to `x_new` isn’t likely to cause any prediction to change, so\n",
    "`(y_new – y_old)` will almost always be 0. \n",
    "\n",
    "In other words, the gradient is 0 almost everywhere. **A very small change in the value of a weight will often not change the accuracy at all.**\n",
    "This means **it is not useful to use accuracy as a loss function. If we do, most of the time our gradients will be 0**, and the model will not be able to learn from that number.\n",
    "\n",
    "**Instead, we need a loss function that, when our weights result in slightly better predictions, gives us a slightly better loss.** So what does a “slightly better prediction” look like, exactly? Well, in this case, it means that if the correct answer is a 3, the score is a little higher, or if the correct answer is a 7, the score is a little lower.\n",
    "\n",
    "**The loss function receives not the images themselves, but the predictions from the model.** So let’s make one argument, `preds`, of values between 0 and 1, where each value is the prediction that an image is a 3. It is a vector (i.e., a rank-1 tensor) indexed over the images.\n",
    "\n",
    "- **The purpose of the loss function is to measure the difference between predicted values and the true values (targets/labels)**\n",
    "- **Loss must be a function that has a meaningful derivative!**\n",
    "- **The loss function is calculated for each item in our dataset, and then at the end of an epoch, the loss values are all averaged and the overall mean is reported for the epoch.**\n",
    "- **It is important that we learn to focus on the metrics, rather than the loss, when judging the performance of a model.**\n",
    "\n",
    "Let’s therefore make another argument, `targets`, with values of 0 or 1 that tells whether an image actually is a 3 or not. It is also a vector (i.e., another rank-1 tensor) indexed over the images.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Suppose we had three images that we knew were 3, 7, 3. And suppose our model predicted with high confidence (0.9) - 3, with slight confidence (0.4) - 7, and with fair confidence (0.2) - 7. This would mean our loss function would receive these values as its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e233a1f2-d80e-4053-ac07-147e7a472079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - class_1, 7 - class_0\n",
    "targets = tensor([1, 0, 1]) # 3, 7, 3\n",
    "preds = tensor([0.9, 0.4, 0.2]) # 3, 7, 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c41f12ef-cdb1-4b52-8d33-e071394f4d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4333)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try a loss that measures the distance between predictions and targets:\n",
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets==1, 1 - predictions, predictions).mean() # use mean because we need a scalar for the loss\n",
    "\n",
    "mnist_loss(preds, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce5c40-3869-4b78-a72d-1fb5b52e05cf",
   "metadata": {},
   "source": [
    "We can see that out loss returns:\n",
    "- low value if prediction is correct and confident\n",
    "- Higher value if the predictions is correct and not so confident \n",
    "- High value if the prediction is incorrect and not so confident \n",
    "- Very high value if the prediction is incorrect and very confident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a968f880-5f71-48ac-8a79-59b49767fb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2333)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's change the last prediction to correct and confident\n",
    "mnist_loss(\n",
    "    tensor([0.9, 0.4, 0.8]), # 3, 7, 3\n",
    "    targets # 3, 7, 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe94587-20df-433d-83e2-2195f12796e1",
   "metadata": {},
   "source": [
    "The loss has decreased. One problem with `mnist_loss` as currently defined is that **it assumes that predictions are always between 0 and 1.** We need to ensure, then, that this is actually the case! As it happens, there is a function that does exactly that . let’s take a look:\n",
    "\n",
    "**Sigmoid**\n",
    "\n",
    "The sigmoid function always outputs a number between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05cccd99-cdd4-41c6-9eb7-f4948c21c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastbook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf678c6-8ca3-4191-99f6-f1b92d9e89c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x): \n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29988678-3ca0-48f5-a278-fbd21a965250",
   "metadata": {},
   "source": [
    "- this function is smooth and goes up (good for the gradients calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c74dd2f7-aea4-44ca-ae76-40150573adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s update mnist_loss to first apply sigmoid to the inputs\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = sigmoid(predictions)\n",
    "    return torch.where(targets==1, 1 - predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02342527-521f-4847-8ae1-f8e03fee78b1",
   "metadata": {},
   "source": [
    "Now we can be confident our loss function will work, even if the predictions are not between 0 and 1. But usually we ge the probability that is always between 0 and 1 (classifier confidence), then compute the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0249308-35cd-4991-a660-2bf34a409f6a",
   "metadata": {},
   "source": [
    "### Gradient Descent and Mini-Batches\n",
    "To take an optimization step, we need to calculate the loss over one or more data items. How many should we use? \n",
    "\n",
    "Usually we average the loss over on n items. The number of data items in the mini-batch is called **batch size**\n",
    "\n",
    "- Mini-Batch solution is good for GPU training\n",
    "- Don't forget to shuffle each mini batch \n",
    "\n",
    "**Large Batch Size**\n",
    "- Get a more accurate and stable estimate of your dataset’s gradients from the loss function\n",
    "- Takes longer to train\n",
    "\n",
    "**Good Batch Size**\n",
    "- Good estimate of the gradients \n",
    "- Optimal training time \n",
    "\n",
    "PyTorch and fastai provide a class that will do the shuffling and mini-batch collation for you, called `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9f562d7-83ad-4f55-9b66-097af035e39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 3, 12,  8, 10,  2]),\n",
       " tensor([ 9,  4,  7, 14,  5]),\n",
       " tensor([ 1, 13,  0,  6, 11])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_elements = range(15)\n",
    "dl = DataLoader(n_elements, batch_size=5, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0084605-867f-4524-b68f-067fd0424a0b",
   "metadata": {},
   "source": [
    "For training a model, we don’t just want any Python collection, but a collection containing independent and dependent variables (the inputs and targets of the model). \n",
    "\n",
    "A **collection that contains tuples of independent and dependent variables is known in PyTorch as a Dataset.**\n",
    "\n",
    "Here’s an example of an extremely simple Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34508fc3-babf-4b7d-9ca2-fe40a063c50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = L(enumerate(string.ascii_lowercase))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de74aa7d-e1a0-49ce-925d-ff87e525ebfc",
   "metadata": {},
   "source": [
    "When we pass a Dataset to a DataLoader we will get back many batches that are themselves tuples of tensors representing batches of independent and dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68c06b17-4a88-41c4-be90-ea12a3fe81c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([19, 14,  0, 24, 20, 12]), ('t', 'o', 'a', 'y', 'u', 'm')),\n",
       " (tensor([23,  8,  9,  3, 16,  6]), ('x', 'i', 'j', 'd', 'q', 'g')),\n",
       " (tensor([ 4,  7,  1, 13,  2, 22]), ('e', 'h', 'b', 'n', 'c', 'w')),\n",
       " (tensor([ 5, 17, 18, 10, 11, 15]), ('f', 'r', 's', 'k', 'l', 'p')),\n",
       " (tensor([25, 21]), ('z', 'v'))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(ds, batch_size=6, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d692e566-56d1-4cfa-9571-e714e323531a",
   "metadata": {},
   "source": [
    "# for each epoch we do\n",
    "for x, y in dl:\n",
    "    pred = model(x)\n",
    "    loss = loss_func(pred, y)\n",
    "    loss.backward()\n",
    "    parameters -= parameters.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fbfc781-3d43-4b39-8534-f03775a3de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init params \n",
    "weigths = init_params((28*28, 1))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d68d5cc-a6d3-4834-9c56-25ef97007257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, create DataLoader from Dataset (train)\n",
    "dl_train = DataLoader(dset_train, batch_size=256)\n",
    "\n",
    "# look at first batch \n",
    "x_dl_train, y_dl_train = first(dl_train)\n",
    "x_dl_train.shape, y_dl_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "faf46b4a-24a8-42e9-894e-692d0b5b4acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataLoader for val data \n",
    "dl_val = DataLoader(dset_val, batch_size=256)\n",
    "\n",
    "# look at first batch \n",
    "x_dl_val, y_dl_val = first(dl_val)\n",
    "x_dl_val.shape, y_dl_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c7e58-2271-4df7-aca9-9502316eeb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
